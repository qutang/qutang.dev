export const meta = {
  type: "mdx",
  title: "ç¬¬ä¸€æœŸï¼šèƒ½å¤Ÿæ£€æµ‹ç»è¡€çš„å«ç”Ÿå·¾ï¼Ÿï¼Ÿï¼Ÿ",
  date: "2019-09-17 10:44:53",
  series: "voyage",
  excerpt: "Ubicomp 2019ï¼Œè°·æ­Œçš„éšç§ç¼–ç ç®—æ³•, ä»¥åŠå…³äºå¼€é¢˜æŠ¥å‘Šçš„æ— å°½æŒ‘æˆ˜ã€‚"
};

import PostLayout from "../../layouts/post";

export const layout = props => (
  <PostLayout {...props} meta={meta}>
    {props.children}
  </PostLayout>
);

ç”±äºæ˜¯ç¬¬ä¸€æœŸæ›´è¿™ä¸ªç³»åˆ—ï¼Œä»‹ç»ä¸‹ç³»åˆ—çš„åˆè¡·ã€‚

ä»å°å–œæ¬¢èˆªæµ·æ¸¸æˆæˆ–è€…è¿™ç§ç±»å‹çš„ç”µå½±å’ŒåŠ¨æ¼«ï¼Œæ¯”å¦‚æµ·å•†ç‹ï¼Œå¤§èˆªæµ·æ—¶ä»£ï¼ŒåŠ å‹’æ¯”æµ·ç›—ä»¥åŠæµ·è´¼ç‹ä¹‹ç±»ï¼Œå¯¹äºä½œä¸ºèˆªæµ·å£«ä¸Šèˆ¹ç„¶åå‡ºæµ·å†’é™©æœ‰ç‰¹åˆ«çš„ç—´è¿·ã€‚ä¸è¿‡è¿™ç§ç—´è¿·ä»…é™äºå¤§èˆªæµ·æ—¶ä»£ï¼Œå·¥ä¸šé©å‘½å°šæœªå¼€å§‹çš„å¸†èˆ¹æ—¶ä»£ã€‚è‡ªä»æœ‰äº†è’¸æ±½æœºï¼Œå«æ˜Ÿå¯¼èˆªï¼Œæ— çº¿ç”µé€šè®¯ï¼Œé“çš®èˆ¹ä»¥åŠåŠè‡ªåŠ¨æˆ–è€…å…¨è‡ªåŠ¨çš„çƒ­å…µå™¨ï¼Œè¶Šå‘è§‰å¾—èˆªæµ·ä¸å†åƒæ˜¯å†’é™©äº†ã€‚

ç”¨èˆªæµ·å½“ä¸»é¢˜ï¼Œæ˜¯æƒ³è¦è®©è‡ªå·±çš„ç»å†æœ‰äº›å†’é™©æ„Ÿå’Œåˆºæ¿€æ„Ÿã€‚

è¿™äº›æ—¥å¿—åˆ†ä¸ºä¸‰ä¸ªæ¿å—ï¼Œ**ç­æœ›å°**åˆ†äº«ä¸ªäººæ–°çš„å‘ç°å’Œè¿›å±•ï¼Œ**ç¯å¡”**åˆ†äº«é‡å¤§çš„ç§‘æŠ€å˜è¿ä»¥åŠä¸ªäººçš„çœ‹æ³•ï¼Œ**ç”²æ¿**åˆ†äº«ä¸ªäººå‘å±•çš„å¿ƒå¾—ä½“ä¼šã€‚

![voyage](../images/voyage.jpg)

## ğŸ”­ç­æœ›å°

ï¼ˆç¼ºä¸€å¼ å¥½çš„å›¾ï¼‰

æœ¬æœˆè·¯è¿‡å¹´åº¦é¢†åŸŸé¡¶ä¼šUbicompï¼Œåœ¨è‹±å›½ä¼¦æ•¦å¬å¼€ã€‚èŠ±äº†ä¸€ç‚¹æ—¶é—´æŠŠkeynoteå’Œç›¸å…³è®ºæ–‡ã€‚ã€‚çš„æ‘˜è¦çœ‹äº†çœ‹ã€‚ç›¸å…³è®ºæ–‡æ­£æ–‡è¯·å¤åˆ¶ç„¶åè‡ªè¡Œæœç´¢ã€‚

### éæ ¸å¿ƒè®ºæ–‡

å–œæ¬¢çš„

* **Evolving Needs in IoT Control and Accountability: A Longitudinal Study on Smart Home Intelligibility**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  A key issue for smart home systems is supporting non-expert users in their management. Whereas feedback design on use cases (such as energy feedback) have gained attention, current approaches to providing awareness on the system state typically provide a rather technical view. Long-term investigations of the practices and resources needed for maintaining Do-It-Yourself smart home systems, are particularly scarce. We report on a design case study in which we equipped 12 households with DIY smart home systems for two years and studied participants' strategies for maintaining system awareness, from learning about its workings to monitoring its behavior. We find that people's needs regarding system accountability changed over time. Their privacy needs were also affected over the same period. We found that participants initially looked for in-depth awareness information from the dedicated web-based dashboard. In the later phases of appropriation, however, their interaction and information needs shifted towards management by exception on mobile or ambient displays -- only focusing on the system when things were 'going wrong'. In terms of system accountability, we find that a system's self-declaration should focus on being socially meaningful rather than technically complete, for instance by relating itself to people's activities and the home routines.
  </details>
  
  *ç®€è¯„ï¼šæ™ºèƒ½å®¶å±…çš„å¯å®šåˆ¶æ€§å¯¹ä¸åŒå®¶åº­çš„ä¸ªæ€§åŒ–éœ€æ±‚éå¸¸é‡è¦ã€‚è€Œè¿™ç§ä¸ªæ€§åŒ–éœ€æ±‚ä¼šéšç€æ—¶é—´å˜åŒ–ã€‚é¢å‘éä¸“ä¸šç”¨æˆ·çš„å¯DIYçš„æ™ºèƒ½å®¶å±…ç³»ç»Ÿè®¾è®¡æ˜¯éš¾ç‚¹ã€‚æœ¬æ–‡çš„ä»·å€¼åœ¨äºè·Ÿè¸ªäº†12ä¸ªç”¨æˆ·ä¸¤å¹´çš„ä½¿ç”¨ä¹ æƒ¯ï¼Œå‘ç°äº†ç”¨æˆ·éœ€æ±‚çš„è¿ç§»ï¼Œä»åˆæœŸé¢é¢ä¿±åˆ°çš„DIYï¼Œåˆ°ä»…ä»…å…³æ³¨â€ç³»ç»ŸçŠ¯é”™â€ã€‚ä¸ªäººè§‰å¾—è¿™ä¸ªè§‚å¯Ÿå¾ˆç¬¦åˆå¸¸ç†ã€‚æˆ‘çš„ç‚’è‚¡ç»å†å‘Šè¯‰æˆ‘ï¼ŒåˆæœŸæ¶¨è·Œæˆ‘éƒ½å…³æ³¨ï¼ŒåæœŸé™¤éæ˜¯å¤§æ¶¨æˆ–è€…å¤§è·Œæˆ‘ç»Ÿç»Ÿä¸careã€‚*

* **micro-Stress EMA: A Passive Sensing Framework for Detecting in-the-wild Stress in Pregnant Mothers**
  
  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  High levels of stress during pregnancy increase the chances of having a premature or low-birthweight baby. Perceived self-reported stress does not often capture or align with the physiological and behavioral response. But what if there was a self-report measure that could better capture the physiological response? Current perceived stress self-report assessments require users to answer multi-item scales at different time points of the day. Reducing it to one question, using microinteraction-based ecological momentary assessment (micro-EMA, collecting a single in situ self-report to assess behaviors) allows us to identify smaller or more subtle changes in physiology. It also allows for more frequent responses to capture perceived stress while at the same time reducing burden on the participant. We propose a framework for selecting the optimal micro-EMA that combines unbiased feature selection and unsupervised Agglomerative clustering. We test our framework in 18 women performing 16 activities in-lab wearing a Biostamp, a NeuLog, and a Polar chest strap. We validated our results in 17 pregnant women in real-world settings. Our framework shows that the question "How worried were you?" results in the highest accuracy when using a physiological model. Our results provide further in-depth exposure to the challenges of evaluating stress models in real-world situations.
  </details>

  *ç®€è¯„ï¼šè€æ¿å–œæ¬¢çš„ã€‚å› ä¸ºmicro-EMAå°±æ˜¯æˆ‘å®éªŒå®¤æœ€å…ˆæå‡ºæ¥çš„ã€‚è¿™ä¸ªæŠ€æœ¯æ½œåŠ›æŒºå¤§çš„ï¼Œå°¤å…¶å½“æ™ºèƒ½æ‰‹è¡¨è¶Šæ¥è¶Šæ™®åŠä¹‹å*

* **Your Table Can Be an Input Panel: Acoustic-based Device-Free Interaction Recognition**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  This paper explores the possibility of extending the input and interactions beyond the small screen of the mobile device onto ad hoc adjacent surfaces, e.g., a wooden tabletop with acoustic signals. While the existing finger tracking approaches employ the active acoustic signal with a fixed frequency, our proposed system Ipanel employs the acoustic signals generated by sliding of fingers on the table for tracking. Different from active signal tracking, the frequency of the finger-table generated acoustic signals keeps changing, making accurate tracking much more challenging than the traditional approaches with fix frequency signal from the speaker. Unique features are extracted by exploiting the spatio-temporal and frequency domain properties of the generated acoustic signals. The features are transformed into images and then we employ the convolutional neural network (CNN) to recognize the finger movement on the table. Ipanel is able to support not only commonly used gesture (click, flip, scroll, zoom, etc.) recognition, but also handwriting (10 numbers and 26 alphabets) recognition at high accuracies. We implement Ipanel on smartphones, and conduct extensive real environment experiments to evaluate its performance. The results validate the robustness of Ipanel, and show that it maintains high accuracies across different users with varying input behaviours (e.g., input strength, speed and region). Further, Ipanel's performance is robust against different levels of ambient noise and varying surface materials.
  </details>

  *ç®€è¯„ï¼šä¸‡ç‰©çš†ä¿¡å·ï¼Œåªè¦æœ‰è„‘æ´ã€‚æ•²æ¡Œå­å¯èƒ½æœ‰ç‚¹å¥‡æ€ªï¼Œä½†æ˜¯æ‹æ‹æ‰‹ï¼Œæ‰“ä¸ªå“æŒ‡è¿˜æ˜¯å¯ä»¥çš„ã€‚*

* **Just-in-Time but Not Too Much: Determining Treatment Timing in Mobile Health**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  There is a growing scientific interest in the use and development of just-in-time adaptive interventions in mobile health. These mobile interventions typically involve treatments, such as reminders, activity suggestions and motivational messages, delivered via notifications on a smartphone or a wearable to help users make healthy decisions in the moment. To be effective in influencing health, the combination of the right treatment and right delivery time is likely critical. A variety of prediction/detection algorithms have been developed with the goal of pinpointing the best delivery times. The best delivery times might be times of greatest risk and/or times at which the user might be most receptive to the treatment notifications. In addition, to avoid over burdening users, there is of ten a constraint on the number of treatments that should be provided per time interval (e.g., day or week). Yet there may be many more times at which the user is predicted or detected to be at risk and/or receptive. The goal then is to spread treatment uniformly across all of these times. In this paper, we introduce a method that spreads the treatment uniformly across the delivery times. This method can also be used to provide data for learning whether the treatments are effective at the delivery times. This work is motivated by our work on two mobile health studies, a smoking cessation study and a physical activity study.
  </details>

  *ç®€è¯„ï¼šè¿™ç¯‡æ–‡ç« æå‡ºçš„é—®é¢˜å¾ˆæ ¸å¿ƒï¼Œæ€ä¹ˆè®¾è®¡æ°å¥½åˆé€‚çš„â€œåŠæ—¶å¹²é¢„â€ã€‚ä½†æ˜¯ï¼Œè¿™ä¸ªabstractæ²¡è®©æˆ‘çœ‹åˆ°å¤ªå¤šæ–°ä¸œè¥¿ã€‚å®ƒçš„æ ¸å¿ƒæ˜¯æŠŠå¹²é¢„å¹³å‡åœ°åˆ†é…åˆ°æ‰€æœ‰æ—¶é—´ä¸Šï¼Œè¿™ä¸ªã€‚ã€‚ã€‚ä¸æ˜¯å¾ˆä¸è¨€è€Œå–»çš„äº‹å—ï¼Ÿä½†æ˜¯è¿™ä¸ªé—®é¢˜ç¡®å®æ˜¯æ‰€æœ‰mobile healthç±»åº”ç”¨çš„æ ¸å¿ƒ*

* **Interrupting Drivers for Interactions: Predicting Opportune Moments for In-vehicle Proactive Auditory-verbal Tasks**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  Auditory-verbal interactions with in-vehicle information systems have become increasingly popular for improving driver safety because they obviate the need for distractive visual-manual operations. This opens up new possibilities for enabling proactive auditory-verbal services where intelligent agents proactively provide contextualized recommendations and interactive decision-making. However, prior studies have warned that such interactions may consume considerable attentional resources, thus negatively affecting driving performance. This work aims to develop a machine learning model that can find opportune moments for the driver to engage in proactive auditory-verbal tasks by using the vehicle and environment sensor data. Given that there is a lack of definition about what constitutes interruptibility for auditory-verbal tasks, we first define interruptible moments by considering multiple dimensions and then iteratively develop the experimental framework through an extensive literature review and four pilot studies. We integrate our framework into OsmAnd, an open-source navigation service, and perform a real-road field study with 29 drivers to collect sensor data and user responses. Our machine learning analysis shows that opportune moments for interruption can be conservatively inferred with an accuracy of 0.74. We discuss how our experimental framework and machine learning models can be used to design intelligent auditory-verbal services in practical deployment contexts.
  </details>

  *ç®€è¯„ï¼šå‚è€ƒä¸Šç¯‡ï¼Œit's all about timingæ—¶æœºå¾ˆé‡è¦ï¼*

* **How Does a Nation Walk?: Interpreting Large-Scale Step Count Activity with Weekly Streak Patterns**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  Activity trackers are being deployed in large-scale physical activity intervention programs, but analyzing their data is difficult due to the large data size and complexity. As such large datasets of steps become more available, it is paramount to develop analysis methods to deeply interpret them to understand the variety and changing nature of human steps behavior. In this work, we explored ways to analyze the heterogeneous steps activity data and propose a framework of dimensions and time aggregations to interpret how providing a city-wide population with activity trackers, and monetary incentives influences their wearing and steps behavior. We analyzed the daily step counts of 140,000 individuals, walking a combined 74 billion steps in 305 days of a city-wide public health campaign. We performed data mining clustering to identify 16 user segments, each with distinctive weekly streaks in patterns of device wear and recorded steps. We demonstrate that these clusters enable us to interpret how some users increased their steps level. Our key contributions are: a new analytic method to scalably interpret large steps data; the insights of our analysis about key user segments in our large intervention; demonstrating the power to predictive user outcomes from their first few days of tracking.
  </details>

  *ç®€è¯„ï¼šç°åœ¨çš„é—®é¢˜ç¡®å®æ˜¯æ•°æ®å¤ªå¤šã€‚è¿™ç§large scaleç ”ç©¶çš„ç‰¹ç‚¹å°±æ˜¯æ–¹æ³•ç®€å•ï¼Œä½†æ˜¯å¾€å¾€èƒ½å¾—åˆ°æœ€æœ‰ç”¨çš„ç»“è®ºã€‚*

ä¸å¤ªå–œæ¬¢çš„

* **Differentiating Higher and Lower Job Performers in the Workplace Using Mobile Sensing**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  Assessing performance in the workplace typically relies on subjective evaluations, such as, peer ratings, supervisor ratings and self assessments, which are manual, burdensome and potentially biased. We use objective mobile sensing data from phones, wearables and beacons to study workplace performance and offer new insights into behavioral patterns that distinguish higher and lower performers when considering roles in companies (i.e., supervisors and non-supervisors) and different types of companies (i.e., high tech and consultancy). We present initial results from an ongoing year-long study of N=554 information workers collected over a period ranging from 2-8.5 months. We train a gradient boosting classifier that can classify workers as higher or lower performers with AUROC of 0.83. *Our work opens the way to new forms of passive objective assessment and feedback to workers to potentially provide week by week or quarter by quarter guidance in the workplace.*
  </details>

  *ç®€è¯„ï¼šçœ‹åå­—ã€‚ã€‚ã€‚emmmmï¼Ÿï¼Ÿï¼Ÿï¼Œå‰å‡ å¤©çœ‹æ–°é—»è¯´æœ‰ä¸­å­¦ç”¨äººè„¸è¯†åˆ«ç›‘æ§è¯¾å ‚è¡¨ç°ï¼Œç°åœ¨åˆæ¥äº†ç”¨mobile sensingç›‘æ§å·¥ä½œè¡¨ç°ï¼Ÿæœ€åä¸€å¥è¯è¯´æ˜¯æä¾›å¸®åŠ©ï¼Œå…¶å®å°±æ˜¯å¸®åŠ©è€æ¿å¼€äººå§ï¼*

* **MenstruLoss: Sensor For Menstrual Blood Loss Monitoring**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  Self-monitoring of menstrual blood loss volume could lead to early detection of multiple gynecological diseases. In this paper, we describe the development of a textile-based blood volume sensor which can be integrated into the sanitary napkin to quantify the menstrual blood loss during menstruation. It is based on sensing the resistance change detected as the output voltage change, with the added volume of fluid. Benchtop characterization tests with 5 mL of fluid determined the effect of spacing, orientation and weight, and location of fluid drop on the sensor. The sensor has been evaluated by intravenous blood samples collected from 18 participants and menstrual blood samples collected from 10 participants for four months. The collected intravenous blood samples and menstrual blood samples were used to create two regression model that can predict the blood volume and menstrual blood volume from the voltage input with Mean Absolute Percentage Error (MAPE) of 11-15% and 15-30% respectively.
  </details>

  *ç®€è¯„ï¼šUbicompæœç„¶æ˜¯è„‘æ´å¤§ä¼šã€‚æœ¬æœŸçƒ­æœæ˜¯å…³çˆ±å¥³æ€§å¥åº·ï¼Œä»æ£€æµ‹æœˆç»å‡ºè¡€é‡å¼€å§‹ã€‚æˆ‘ä»¿ä½›çœ‹åˆ°äº†ç½‘çº¢å¸¦è´§çš„é»‘ç§‘æŠ€å«ç”Ÿå·¾ã€‚ä¸è¿‡ã€‚ã€‚ã€‚é”™è¯¯ç‡å±…ç„¶æœ‰30%è¿™ä¹ˆé«˜ï¼Œè¿™æ›´åƒæ˜¯è¥é”€å·ã€‚*

### æ ¸å¿ƒè®ºæ–‡

ä»Šå¹´å…³äºActivity recognitionç®—æ³•çš„è®ºæ–‡ä¸»è¦å…³æ³¨ç‚¹éƒ½åœ¨annotationçš„é—®é¢˜ï¼Œè¿™ä¹Ÿä¾§é¢ååº”äº†æ•°æ®æ ‡è®°æ˜¯æœ¬é¢†åŸŸçš„ä¸€å¤§éš¾ç‚¹ã€‚

* **On the role of features in human activity recognition**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  Traditionally, the sliding window based activity recognition chain (ARC) has been dominating practical applications, in which features are carefully optimized towards scenario specifics. Recently, end-to-end, deep learning methods, that do not discriminate between representation learning and classifier optimization, have become very popular also for HAR using wearables, promising "out-of-the-box" modeling with superior recognition capabilities. In this paper, we revisit and analyze specifically the role feature representations play in HAR using wearables. In a systematic exploration we evaluate eight different feature extraction methods, including conventional heuristics and recent representation learning methods, and assess their capabilities for effective activity recognition on five benchmarks. Optimized feature learning integrated into the conventional ARC leads to comparable if not better recognition results as if using end-to-end learning methods, while at the same time offering practitioners more flexibility to optimize their systems towards specifics of wearables and their constraints and limitations.
  </details>

  *ç®€è¯„ï¼šè¿™æ˜¯ä¸€ç¯‡æˆ‘éœ€è¦é˜…è¯»çš„æ–‡ç« ï¼Œæ‰€ä»¥ä»·å€¼ä¸è¨€è€Œå–»ã€‚æ·±åº¦å­¦ä¹ å’Œä¼ ç»Ÿæœºå™¨å­¦ä¹ åœ¨Activity recognitioné¢†åŸŸè¡¨ç°å·®åˆ«ä¸å¤§ã€‚æ·±åº¦å­¦ä¹ å¹¶æ²¡æœ‰åƒåœ¨è®¡ç®—æœºè§†è§‰æˆ–è€…è¯­éŸ³è¯­è¨€å¤„ç†ä¸­é‚£æ ·ï¼Œè¡¨ç°å‡ºå·¨å¤§çš„ä¼˜åŠ¿ã€‚æœ€è¿‘ç»“æŸçš„Sussex-Huawei Locomotion Dataset challenge 2019çš„æ€»ç»“è®ºæ–‡é‡Œä¹Ÿå°è¯äº†ç±»ä¼¼çš„çœ‹æ³•[^1]ã€‚ç©¶å…¶åŸå› ï¼Œä¸ªäººè®¤ä¸ºè¿˜æ˜¯åœ¨featureçš„é—®é¢˜ä¸Šï¼Œæ·±åº¦å­¦ä¹ çš„ä¸€å¤§ç‰¹ç‚¹å°±æ˜¯å¤šå±‚ç‰¹å¾çš„è‡ªåŠ¨æå–ã€‚æŒ‰ç†è¯´activity recognitionè·Ÿè¯­éŸ³ä¹‹ç±»çš„æ—¶åŸŸä¿¡å·æœ‰é‚£ä¹ˆç‚¹ç›¸ä¼¼æ€§ï¼Œæ·±åº¦å­¦ä¹ çš„è¡¨ç°ä¸åº”è¯¥è¿™ä¹ˆå·®æ‰å¯¹ã€‚åªèƒ½è¯´å¯èƒ½è¿åŠ¨ä¿¡å·éƒ½å¤ªä½é¢‘ï¼Œåœ¨çŸ­æ—¶é—´å†…åŸºæœ¬ä¸Šéƒ½æ˜¯non-stationaryçš„ã€‚*

* **Leveraging Active Learning and Conditional Mutual Information to Minimize Data Annotation in Human Activity Recognition**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  A difficulty in human activity recognition (HAR) with wearable sensors is the acquisition of large amounts of annotated data for training models using supervised learning approaches. While collecting raw sensor data has been made easier with advances in mobile sensing and computing, the process of data annotation remains a time-consuming and onerous process. This paper explores active learning as a way to minimize the labor-intensive task of labeling data. We train models with active learning in both offline and online settings with data from 4 publicly available activity recognition datasets and show that it performs comparably to or better than supervised methods while using around 10% of the training data. Moreover, we introduce a method based on conditional mutual information for determining when to stop the active learning process while maximizing recognition performance. This is an important issue that arises in practice when applying active learning to unlabeled datasets.
  </details>

  *ç®€è¯„ï¼šè¿™ç¯‡æ–‡ç« è¢«æˆ‘å®éªŒå®¤ç»Ÿä¸€æ¨èä¸ºå¿…è¯»ã€‚æˆ‘è‰è‰è¿‡äº†ä¸€éï¼Œç¡®å®ä¸é”™ï¼Œæœ‰å¾ˆè¯¦ç»†çš„ç®—æ³•æè¿°ä»¥åŠå®éªŒè®¾è®¡ã€‚æœ€å…³é”®çš„æ˜¯ç»“æœå¾ˆæŒ¯å¥‹ï¼Œé€šè¿‡ç­›é€‰è®­ç»ƒæ•°æ®ï¼Œå¯ä»¥åªç”¨10%çš„æ•°æ®è¾¾åˆ°ä½¿ç”¨å®Œæ•´æ•°æ®é›†æ—¶çš„è¡¨ç°ã€‚ä»å¦ä¸€æ–¹ä¹Ÿèƒ½è¯´æ˜å®é™…ä¸Šactivity recognitionçš„æ•°æ®é›†é‡Œå­˜åœ¨ç€å¤§é‡çš„å†—ä½™æ•°æ®ã€‚è¿™è·Ÿæˆ‘å°†è¦è¿›è¡Œçš„å®éªŒä¸è°‹è€Œåˆï¼Œä¸€ä¸ªåŸºæœ¬å‡è®¾æ˜¯å…³äºwalkingï¼Œæ¥è‡ªåŒä¸€episodeçš„æ•°æ®å¹¶ä¸éœ€è¦å¾ˆå¤šï¼Œä¸¤ä¸‰ä¸ªåº”è¯¥å°±å¤Ÿäº†ï¼Œå…³é”®åœ¨äºæä¾›è¶³å¤Ÿä¸°å¯Œçš„episodeã€‚è¿™ç¯‡æ–‡ç« é—´æ¥è¯å®äº†è¿™ç§å‡è®¾ã€‚*

* **Handling annotation uncertainty in human activity recognition**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  Developing systems for Human Activity Recognition (HAR) using wearables typically relies on datasets that were manually annotated by human experts with regards to precise timings of instances of relevant activities. However, obtaining such data annotations is often very challenging in the predominantly mobile scenarios of Human Activity Recognition. As a result, labels often carry a degree of uncertainty-label jitter-with regards to: i) correct temporal alignments of activity boundaries; and ii) correctness of the actual label provided by the human annotator. In this work, we present a scheme that explicitly incorporates label jitter into the model training process. We demonstrate the effectiveness of the proposed method through a systematic experimental evaluation on standard recognition tasks for which our method leads to significant increases of mean F1 scores.
  </details>

  *ç®€è¯„ï¼šæˆ‘æ˜¯çœŸçš„ä¸å–œæ¬¢ä¸ç›´æ¥åœ¨æ‘˜è¦é‡Œå†™åˆ°åº•æ¨¡å‹è¡¨ç°æå‡äº†å¤šå°‘çš„è®ºæ–‡çš„ï¼ˆå·®è¯„+1ï¼‰ï¼ä½†æ˜¯é‰´äºæœ¬æ–‡è¯´äº†ä¸ªå¾ˆé‡è¦çš„é—®é¢˜ï¼Œå°±æ˜¯æ•°æ®æ ‡è®°æ—¶çš„æ ‡è®°å’Œæ•°æ®çš„æ—¶é—´æˆ³ç»å¸¸å‡ºç°é”™ä½ã€‚ä½œè€…ä¹Ÿä¸è¯´åˆ°åº•ç”¨äº†ä»€ä¹ˆæ–¹æ³•è§£å†³è¿™ä¸ªé—®é¢˜ï¼ˆå·®è¯„+1ï¼‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬é€šå¸¸éƒ½æ˜¯æå¤´å»å°¾å»æ‰é”™ä½çš„éƒ¨åˆ†ï¼Œä¸è¿‡è¿™ç§æ–¹æ³•ä¸èƒ½å¾ˆå¥½çš„è§£å†³çŸ­æ—¶é—´çš„åŠ¨ä½œï¼Œå› ä¸ºæç€æç€å°±æ²¡æœ‰æ•°æ®äº†ã€‚*

* **Swimming style recognition and lap counting using a smartwatch and deep learning**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  Human activity recognition from raw sensor data has enabled modern wearable devices to track and analyze everyday activities. However, when used in real world conditions, the performance of off-the-shelf devices is often insufficient. This paper tackles the problem of swimming style recognition and lap counting using sensor data from a single smartwatch. In total 17 hours of this data was collected from 40 swimmers of diverse backgrounds. The data was then used to train a convolutional neural network to recognize the four main swimming styles, transition periods and lap turns. Our method achieves an F1 score of 97.4% for style recognition and 99.2% for counting laps. To the best of our knowledge, these results are the first to enable accurate automatic swimming recognition in a realistic and completely uncontrolled environment.
  </details>

  *ç®€è¯„ï¼šæˆ‘ä»¿ä½›çœ‹åˆ°äº†ä¸‹ä¸€ä»£apple watchçš„å–ç‚¹*

* **Estimating load positions of wearable devices based on difference in pulse wave arrival time**

  <details>
  <summary>
    Open to see abstract ğŸ‘‡
  </summary>
  With the increasing use of wearable devices equipped with various sensors, human activities, biometric information, and surrounding situations can be obtained via sensor data regardless of time and place. When position-free wearable devices are attached to an arbitrary part of the body, the attached position should be identified because the application process changes relative to the position. For systems that use multiple wearable devices to capture body-wide movement, estimating the attached position of the devices is meaningful. Most conventional studies estimate the loading position of the sensor using accelerometer and gyroscope data; therefore, users must perform specific motions so that each sensor produces values unique to the given position. We propose a method that estimates the load position of wearable devices without forcing the wearer to perform specific actions. The proposed method estimates the time difference between a heartbeat obtained by an electrocardiogram and a pulse wave obtained using a pulse sensor and classifies the sensor position from the estimated time difference. We assume that pulse sensor is embedded in the wearable devices to be attached to the user. From the results of an evaluation experiment with five subjects, an average F-measure of 0.805 was achieved over 15 body parts. The left ear and the right finger achieved an F-measure of 0.9+ when the proposed system uses data of approximately 20 seconds as an input.
  </details>

  *ç®€è¯„ï¼šæ—¥æœ¬äººçš„è„‘æ´è¿˜æ˜¯å¯ä»¥çš„ã€‚ä»ç›´è§‰ä¸Šè®²ï¼Œè¿™çš„ç¡®å¯èƒ½æ˜¯ä¸ªè›®æœ‰æ•ˆçš„æ£€æµ‹ä¼ æ„Ÿå™¨ä½ç½®çš„æ–¹æ³•ã€‚ä½†æ˜¯å¦‚æœè¿™ä¸œè¥¿æ²¡æˆ´åœ¨çš®è‚¤è¡¨é¢å’‹åŠå‘¢ï¼Ÿï¼Ÿï¼Ÿï¼Ÿ*

## â›¯ç¯å¡”

æœ¬æœˆæ–°é—»ä¸å¤šï¼Œä½†æ˜¯è‹¹æœçš„æ–°å“å‘å¸ƒä¼šç¡®å®èµšè¶³äº†çœ¼çƒã€‚åœ¨è®¡ç®—æœºç§‘å­¦é¢†åŸŸï¼Œä¸å¾—ä¸æçš„æ˜¯è°·æ­Œçš„[differential privacy](https://github.com/google/differential-privacy)ç®—æ³•ã€‚ä¸ªäººè§‰å¾—è°·æ­Œå¼€å‘è¿™ä¸ªç®—æ³•çš„æ ¹æœ¬ç›®çš„å°±æ˜¯ç»•å¼€æ¬§ç›Ÿçš„æ•°æ®ä¿æŠ¤GDPRæ³•æ¡ˆçš„ç›‘ç®¡ã€‚æ¯•ç«Ÿè°·æ­Œæ˜¯ä»¥æ•°æ®ä¸ºç”Ÿçš„å…¬å¸å‘€ã€‚

è‹¹æœåšäº§å“çš„æ€åº¦æ˜¯çœŸçš„å€¼å¾—å­¦ä¹ ï¼Œä¸è·Ÿé£ï¼Œä»¥ç”¨æˆ·çš„æ ¸å¿ƒä½“éªŒä¸ºå¯¼å‘ã€‚æŠ€æœ¯æ–¹é¢ï¼Œä¸‰æ‘„åƒå¤´çš„æ— ç¼æµç•…åˆ‡æ¢å€¼å¾—å¹ä¸€æ³¢ï¼Œå¸Œæœ›å›½äº§å‚å•†ç»§ç»­æ½œå¿ƒä¿®ç‚¼ï¼Œå°¤å…¶æ˜¯è¢«å¯„äºˆä¼—æœ›çš„åä¸ºã€‚

## âš“ç”²æ¿

æ— å°½åœ°ã€‚ã€‚ã€‚ä¸ã€‚ã€‚ã€‚å¼€é¢˜æŠ¥å‘Šå’Œè€æ¿åšå¼ˆçš„è¿‡ç¨‹ä¸­ã€‚ã€‚ã€‚

### å…³äºå¼€é¢˜æŠ¥å‘Šçš„äº›å¾®è¿›å±•

[^1]: L. Wang, H. Gjoreski, K. Murao, T. Okita, D. Roggen. â€œSummary of the Sussex-Huawei Locomotion-Transportation Recognition Challenge.â€ In Ubicomp Adjunct Proceedings. ACM, 2018.
