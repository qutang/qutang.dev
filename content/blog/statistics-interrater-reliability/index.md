今天（2018 年 4 月 15 日）在看一篇论文（关于电子病历应用中的问题）时偶然看到了这个术语，感觉这是个挺容易被忽略的概念，尤其对于我们工程学科（非医学，生物或者统计学）的学生来说。我们平时实验里会用相关性系数(correlation coefficient)来测试特征间的相关性，但是从实验设计的角度，我们经常忽略从严格地统计学角度比较不同模型表现的问题。这种表现可能是因为模型的差异，也可能是因为开发或者测试模型所用数据的问题，或者甚至是实验数据收集方式的问题。Interrater reliability 就是用不同数据收集者之间一致性来度量数据可信度的一种概念。

### 一致性百分比（Percentage Agreement）

举个例子，甲跟乙分别对 100 个 🍎 的质量进行评价，怎么判断他们的评价是不是客观？ 一种方法就是比较他们一致的评价占整个数量的百分比，这个叫做 Percentage Agreement。假设甲认为 80 个苹果是好的，乙只认为其中 70 个是好的，那么他们的 Percentage Agreement 就是 0.9。有人会问为嘛不是 0.7，因为在坏苹果中，有 20 个甲乙的评价都是坏的，所以甲乙之间一致性的评价有 90 个（可以是好苹果也可以是坏苹果）。

$$Pr(a) = 90 / 100 = 0.9$$

$a$ 这里表示 actual agreement,也就是一致评价的总量。

### Cohen's Kappa

后来有个统计学家 Cohen 发现了问题，假如这俩人随便瞎猜呢，那么他们针对一个苹果，评价一致的概率就有 $0.5 * 0.5 + 0.5 * 0.5 = 0.5$， 有 50%的可能他们会猜得一样。这样就会导致 Percentage agreement 被高估。所以这人发明了著名的 Cohen's Kappa $\kappa$，相关性系数的一种，来修正被随机性的一致结果高估的 Percentage agreement。具体算法请参见[^1]。但是这个系数也有自己的缺陷，它假定了两个测试者是相互独立互不影响的，并且使用边际概率分布来估计随机性的一致结果的概率（这个是不准确的），所以这种修正也是不精确的。

举个例子，如果甲跟乙师从同一个坑爹师傅，那么可能他们犯错的模式都会很像，造成的结果就是 Percentage agreement 很高，但是 Cohen's Kappa 很低。光从 Cohen's Kappa 一个数值很难反向推断造成这种情况的原因（也可能甲事先看到了乙的评价，造成他的评价不再独立）。

实际应用中，Percentage agreement 和 Cohen's kappa 通常结合起来使用。前者在医学研究中的接受阈值大概是 0.8 而后者为 0.6，低于这个数值的数据通常是不可信的，基于这些数据的推论也就靠不住了。

[^1]: [Interrater reliability: the kappa statistic](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3900052/)